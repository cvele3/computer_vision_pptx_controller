import cv2
import mediapipe as mp
import pyautogui

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# Helper function to count fingers
def detect_gesture(hand_landmarks):
    """
    Detect gesture based on hand landmarks.
    """
    FINGER_TIPS = [8, 12, 16, 20]
    finger_states = []

    for tip in FINGER_TIPS:
        if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y:
            finger_states.append(1)  # Finger is "up"
        else:
            finger_states.append(0)  # Finger is "down"

    # Thumb
    thumb_tip_x = hand_landmarks.landmark[4].x
    thumb_base_x = hand_landmarks.landmark[2].x
    thumb_up = thumb_tip_x < thumb_base_x  # Adjust for left/right hand

    # Gesture mapping
    if thumb_up and sum(finger_states) == 0:
        return "ENTER_PRESENTATION"
    elif sum(finger_states) == 1:
        return "NEXT_SLIDE"
    elif sum(finger_states) == 2:
        return "PREVIOUS_SLIDE"
    elif sum(finger_states) == 3:
        return "PLAY_VIDEO"
    elif sum(finger_states) == 4:
        return "STOP_VIDEO"
    elif sum(finger_states) == 5:
        return "BLANK_SCREEN"
    elif not any(finger_states) and not thumb_up:
        return "EXIT_PRESENTATION"
    else:
        return "UNKNOWN"

# Map gestures to PowerPoint actions
def execute_action(action):
    if action == "ENTER_PRESENTATION":
        print("Entering Presentation Mode")
        pyautogui.press("f5")
    elif action == "NEXT_SLIDE":
        print("Next Slide")
        pyautogui.press("right")
    elif action == "PREVIOUS_SLIDE":
        print("Previous Slide")
        pyautogui.press("left")
    elif action == "PLAY_VIDEO":
        print("Play Video")
        pyautogui.press("space")
    elif action == "STOP_VIDEO":
        print("Stop Video")
        pyautogui.press("space")
    elif action == "BLANK_SCREEN":
        print("Blank Screen")
        pyautogui.press("b")
    elif action == "EXIT_PRESENTATION":
        print("Exiting Presentation Mode")
        pyautogui.press("esc")

# Main function
def main():
    cap = cv2.VideoCapture(0)

    with mp_hands.Hands(
        model_complexity=1,
        max_num_hands=1,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    ) as hands:
        while True:
            success, frame = cap.read()
            if not success:
                continue

            frame = cv2.flip(frame, 1)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = hands.process(frame_rgb)

            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                    # Detect gesture
                    gesture = detect_gesture(hand_landmarks)
                    if gesture != "UNKNOWN":
                        execute_action(gesture)

            cv2.imshow("Hand Gesture Control", frame)
            if cv2.waitKey(5) & 0xFF == ord('q'):
                break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
